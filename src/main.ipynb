{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib as pb\n",
    "\n",
    "# Environment\n",
    "ROOT_PATH = pb.Path('..')\n",
    "DATA_DIR_PATH = ROOT_PATH / 'data'\n",
    "CACHE_DIR_PATH = ROOT_PATH / '.cache'\n",
    "TRANSFORMERS_CACHE_DIR_PATH = CACHE_DIR_PATH / 'transformers'\n",
    "DATASETS_CACHE_DIR_PATH = CACHE_DIR_PATH / 'datasets'\n",
    "TEST_DATA_FILE = DATA_DIR_PATH / 'test_data.csv'\n",
    "TRAIN_DATA_FILE = DATA_DIR_PATH / 'train_data.csv'\n",
    "SUBMISSIONS_DIR_PATH = ROOT_PATH / 'submissions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['TRANSFORMERS_CACHE'] = str(TRANSFORMERS_CACHE_DIR_PATH)\n",
    "os.environ['HF_DATASETS_CACHE'] = str(DATASETS_CACHE_DIR_PATH)\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torchdata\n",
    "import torchtext\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Subset, DataLoader, Dataset\n",
    "from torch import backends\n",
    "import typing\n",
    "import pathlib as pb\n",
    "import os\n",
    "import gc\n",
    "from typing import List, Tuple, Dict, Set, Callable, Any\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import platform\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_available_device, read_data, silence_warnings\n",
    "from preprocess import BERTPreprocessor\n",
    "from data import SexismDataset\n",
    "from models import Args, Output, BertFlatClassModel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use available GPU\n",
    "DEVICE: torch.device = get_available_device()\n",
    "\n",
    "# Deterministic experiments\n",
    "SEED = 61\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "np.random.RandomState(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "# Adjust package settings\n",
    "silence_warnings()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw dataset\n",
    "train_data_raw, test_data_raw = read_data(DATA_DIR_PATH)\n",
    "\n",
    "# Initialize custom pretraiend preprocessor\n",
    "preprocessor = BERTPreprocessor()\n",
    "\n",
    "# Create and preprocess the datasets\n",
    "train_dataset = SexismDataset(train_data_raw, preprocessor)\n",
    "test_dataset = SexismDataset(test_data_raw, preprocessor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep the training setup separately\n",
    "args = Args()\n",
    "model_factory: Callable[[], nn.Module] = lambda: torch.compile(BertFlatClassModel(unfreeze='none')).to(DEVICE)\n",
    "optim_factory: Callable[..., optim.Optimizer] = lambda params: optim.AdamW(params, lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "loss_fn = nn.CrossEntropyLoss(weight=train_dataset.weights.to(DEVICE))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HyperParameter Tuning using K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspired from: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "\n",
    "for i, (train_idx, valid_idx) in enumerate(kf.split(train_dataset, train_dataset.classes)):\n",
    "    print('K-FOLD: {}'.format(i))\n",
    "    \n",
    "    # Reinitialize the model\n",
    "    model: nn.Module = model_factory()\n",
    "    optimizer: optim.Optimizer = optim_factory(model.parameters())\n",
    "\n",
    "    # Split the data\n",
    "    train_subset = Subset(train_dataset, train_idx)\n",
    "    valid_subset = Subset(train_dataset, valid_idx)\n",
    "\n",
    "    # Create the dataloaders\n",
    "    train_loader = DataLoader(train_subset, args.batch_size)\n",
    "    valid_loader = DataLoader(train_subset, args.batch_size)\n",
    "\n",
    "    # ---  Training  ---\n",
    "    for epoch in range(args.num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss: List[float] = []\n",
    "        epoch_accy: List[float] = []\n",
    "\n",
    "        for batch_i, batch in enumerate(train_loader):\n",
    "            # Send batch to GPU\n",
    "            batch: Dict[str, Tensor] = { k: v.to(DEVICE) for k, v in batch.items() }\n",
    "\n",
    "            # Make predictions\n",
    "            y_true: Tensor | np.ndarray = batch['label']\n",
    "            y_pred: Tensor | np.ndarray = model.forward(batch)\n",
    "\n",
    "            # Compute the loss\n",
    "            optimizer.zero_grad()\n",
    "            loss: Tensor = loss_fn(y_pred, y_true)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Compute the accuracy\n",
    "            y_true = y_true.detach().cpu().numpy()\n",
    "            y_pred = y_pred.detach().argmax(dim=1).cpu().numpy()\n",
    "            epoch_accy.append(balanced_accuracy_score(y_true, y_pred))\n",
    "\n",
    "            # Track progress\n",
    "            epoch_loss.append(loss.detach().cpu().numpy())\n",
    "\n",
    "        mean_loss: float = np.array(epoch_loss).mean()\n",
    "        mean_accy: float = np.array(epoch_accy).mean()\n",
    "        print('Train Epoch {} - Loss: {}, Accuracy: {}'.format(epoch, mean_loss, mean_accy))\n",
    "\n",
    "        # --- Validation ---\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on the Whole Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Validation - Loss: {}, Accuracy: {}'.format(output.loss_mean, output.accy_mean))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "labels: List[str] = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_i, batch in enumerate(test_loader):\n",
    "        # Send batch to GPU\n",
    "        batch: Dict[str, Tensor] = { k: v.to(DEVICE) for k, v in batch.items() }\n",
    "\n",
    "        # Make predictions\n",
    "        y_pred: Tensor | np.ndarray = model.forward(batch)\n",
    "\n",
    "        # Compute the accuracy\n",
    "        y_pred = y_pred.detach().argmax(dim=1).cpu().numpy()\n",
    "        y_pred = np.vectorize(train_dataset.class_to_label.get)(y_pred)\n",
    "\n",
    "        # Concatenate the results\n",
    "        labels.extend(y_pred.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output: pd.DataFrame = pd.DataFrame({ 'Label': pd.Series(data=labels) })\n",
    "output = output.reset_index()\n",
    "output = output.rename(columns={ 'index': 'Id' })\n",
    "output.to_csv(SUBMISSIONS_DIR_PATH / 'submission_8.csv', index=False)\n",
    "output['Label'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free memory\n",
    "# del model\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nitro-lang-processing-2-mFR9zZTH-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
