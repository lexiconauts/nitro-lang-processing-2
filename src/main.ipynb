{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib as pb\n",
    "\n",
    "# Environment\n",
    "ROOT_PATH = pb.Path('..')\n",
    "DATA_DIR_PATH = ROOT_PATH / 'data'\n",
    "CACHE_DIR_PATH = ROOT_PATH / '.cache'\n",
    "TRANSFORMERS_CACHE_DIR_PATH = CACHE_DIR_PATH / 'transformers'\n",
    "DATASETS_CACHE_DIR_PATH = CACHE_DIR_PATH / 'datasets'\n",
    "TEST_DATA_FILE = DATA_DIR_PATH / 'test_data.csv'\n",
    "TRAIN_DATA_FILE = DATA_DIR_PATH / 'train_data.csv'\n",
    "SUBMISSIONS_DIR_PATH = ROOT_PATH / 'submissions'\n",
    "\n",
    "# Model Repositories\n",
    "BERT_RO_MODEL_REPO = 'dumitrescustefan/bert-base-romanian-cased-v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['TRANSFORMERS_CACHE'] = str(TRANSFORMERS_CACHE_DIR_PATH)\n",
    "os.environ['HF_DATASETS_CACHE'] = str(DATASETS_CACHE_DIR_PATH)\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/invokariman/Projects/git/nitro-lang-processing-2/.cache/pypoetry/virtualenvs/nitro-lang-processing-2-mFR9zZTH-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torchdata\n",
    "import torchtext\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Subset, DataLoader, Dataset\n",
    "from torch import backends\n",
    "import typing\n",
    "import pathlib as pb\n",
    "import os\n",
    "import gc\n",
    "from typing import List, Tuple, Dict, Set\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import platform\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_available_device, read_data\n",
    "from preprocess import BertPreprocessor\n",
    "from data import SexismDataset\n",
    "from models import BertFlatClassModel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use available GPU\n",
    "DEVICE: torch.device = get_available_device()\n",
    "\n",
    "# Deterministic experiments\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "np.random.RandomState(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_raw, test_data_raw = read_data(DATA_DIR_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SexismDataset(train_data_raw, BertPreprocessor(train_data_raw, BERT_RO_MODEL_REPO))\n",
    "test_dataset = SexismDataset(test_data_raw, BertPreprocessor(test_data_raw, BERT_RO_MODEL_REPO))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training specifications\n",
    "batch_size = 8\n",
    "num_epochs = 10\n",
    "num_workers = 1\n",
    "learning_rate = 5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dumitrescustefan/bert-base-romanian-cased-v1 were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Instantiate language model\n",
    "# Inspired from: https://luv-bansal.medium.com/fine-tuning-bert-for-text-classification-in-pytorch-503d97342db2\n",
    "model: nn.Module = torch.compile(BertFlatClassModel(BERT_RO_MODEL_REPO, unfreeze='all')).to(DEVICE)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss() # TODO: weighted loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shutup Scikit-Learn Warnings\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-03-25 19:13:00,698] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 0 - Loss: 0.6729198694229126, Accuracy: 0.5892546031746031\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39m# Make predictions\u001b[39;00m\n\u001b[1;32m     30\u001b[0m y_true: Tensor \u001b[39m|\u001b[39m np\u001b[39m.\u001b[39mndarray \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> 31\u001b[0m y_pred: Tensor \u001b[39m|\u001b[39m np\u001b[39m.\u001b[39mndarray \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mforward(batch)\n\u001b[1;32m     33\u001b[0m \u001b[39m# Compute the loss\u001b[39;00m\n\u001b[1;32m     34\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/Projects/git/nitro-lang-processing-2/.cache/pypoetry/virtualenvs/nitro-lang-processing-2-mFR9zZTH-py3.10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:82\u001b[0m, in \u001b[0;36mOptimizedModule.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 82\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdynamo_ctx(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_orig_mod\u001b[39m.\u001b[39;49mforward)(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Projects/git/nitro-lang-processing-2/.cache/pypoetry/virtualenvs/nitro-lang-processing-2-mFR9zZTH-py3.10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:209\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m dynamic_ctx\u001b[39m.\u001b[39m\u001b[39m__enter__\u001b[39m()\n\u001b[1;32m    208\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 209\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    210\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m~/Projects/git/nitro-lang-processing-2/src/models.py:42\u001b[0m, in \u001b[0;36mBertFlatClassModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[39mif\u001b[39;00m param_name \u001b[39min\u001b[39;00m layers:\n\u001b[1;32m     40\u001b[0m                 param\u001b[39m.\u001b[39mrequires_grad \u001b[39m=\u001b[39m unfreeze\n\u001b[0;32m---> 42\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Dict[\u001b[39mstr\u001b[39m, Tensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m     43\u001b[0m     \u001b[39m# Extract the relevant data\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     input_ids \u001b[39m=\u001b[39m x[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     45\u001b[0m     attention_mask \u001b[39m=\u001b[39m x[\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/Projects/git/nitro-lang-processing-2/.cache/pypoetry/virtualenvs/nitro-lang-processing-2-mFR9zZTH-py3.10/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:209\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m dynamic_ctx\u001b[39m.\u001b[39m\u001b[39m__enter__\u001b[39m()\n\u001b[1;32m    208\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 209\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    210\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m~/Projects/git/nitro-lang-processing-2/.cache/pypoetry/virtualenvs/nitro-lang-processing-2-mFR9zZTH-py3.10/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:2819\u001b[0m, in \u001b[0;36maot_module_simplified.<locals>.forward\u001b[0;34m(*runtime_args)\u001b[0m\n\u001b[1;32m   2817\u001b[0m full_args\u001b[39m.\u001b[39mextend(params_flat)\n\u001b[1;32m   2818\u001b[0m full_args\u001b[39m.\u001b[39mextend(runtime_args)\n\u001b[0;32m-> 2819\u001b[0m \u001b[39mreturn\u001b[39;00m compiled_fn(full_args)\n",
      "File \u001b[0;32m~/Projects/git/nitro-lang-processing-2/.cache/pypoetry/virtualenvs/nitro-lang-processing-2-mFR9zZTH-py3.10/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1222\u001b[0m, in \u001b[0;36mmake_boxed_func.<locals>.g\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m   1221\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mg\u001b[39m(args):\n\u001b[0;32m-> 1222\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[0;32m~/Projects/git/nitro-lang-processing-2/.cache/pypoetry/virtualenvs/nitro-lang-processing-2-mFR9zZTH-py3.10/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:2386\u001b[0m, in \u001b[0;36maot_dispatch_autograd.<locals>.debug_compiled_function\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   2380\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m can_require_grad:\n\u001b[1;32m   2381\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m a\u001b[39m.\u001b[39mrequires_grad, format_guard_bug_msg(\n\u001b[1;32m   2382\u001b[0m             aot_config,\n\u001b[1;32m   2383\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mdescribe_input(i,\u001b[39m \u001b[39maot_config)\u001b[39m}\u001b[39;00m\u001b[39m would not require grad\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   2384\u001b[0m         )\n\u001b[0;32m-> 2386\u001b[0m \u001b[39mreturn\u001b[39;00m compiled_function(\u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[0;32m~/Projects/git/nitro-lang-processing-2/.cache/pypoetry/virtualenvs/nitro-lang-processing-2-mFR9zZTH-py3.10/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1898\u001b[0m, in \u001b[0;36mcreate_runtime_wrapper.<locals>.runtime_wrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   1895\u001b[0m     args_with_synthetic_bases \u001b[39m=\u001b[39m args\n\u001b[1;32m   1897\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39m_force_original_view_tracking(\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m-> 1898\u001b[0m     all_outs \u001b[39m=\u001b[39m call_func_with_args(\n\u001b[1;32m   1899\u001b[0m         compiled_fn,\n\u001b[1;32m   1900\u001b[0m         args_with_synthetic_bases,\n\u001b[1;32m   1901\u001b[0m         disable_amp\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1902\u001b[0m     )\n\u001b[1;32m   1904\u001b[0m num_mutated_inps \u001b[39m=\u001b[39m runtime_metadata\u001b[39m.\u001b[39mnum_mutated_inputs\n\u001b[1;32m   1905\u001b[0m num_metadata_mutated_inps \u001b[39m=\u001b[39m runtime_metadata\u001b[39m.\u001b[39mnum_mutated_metadata_inputs\n",
      "File \u001b[0;32m~/Projects/git/nitro-lang-processing-2/.cache/pypoetry/virtualenvs/nitro-lang-processing-2-mFR9zZTH-py3.10/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1247\u001b[0m, in \u001b[0;36mcall_func_with_args\u001b[0;34m(f, args, steal_args, disable_amp)\u001b[0m\n\u001b[1;32m   1245\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1246\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(f, \u001b[39m\"\u001b[39m\u001b[39m_boxed_call\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 1247\u001b[0m         out \u001b[39m=\u001b[39m normalize_as_list(f(args))\n\u001b[1;32m   1248\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1249\u001b[0m         \u001b[39m# TODO: Please remove soon\u001b[39;00m\n\u001b[1;32m   1250\u001b[0m         \u001b[39m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001b[39;00m\n\u001b[1;32m   1251\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1252\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mYour compiler for AOTAutograd is returning a a function that doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt take boxed arguments. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1253\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1254\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1255\u001b[0m         )\n",
      "File \u001b[0;32m~/Projects/git/nitro-lang-processing-2/.cache/pypoetry/virtualenvs/nitro-lang-processing-2-mFR9zZTH-py3.10/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1222\u001b[0m, in \u001b[0;36mmake_boxed_func.<locals>.g\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m   1221\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mg\u001b[39m(args):\n\u001b[0;32m-> 1222\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[0;32m~/Projects/git/nitro-lang-processing-2/.cache/pypoetry/virtualenvs/nitro-lang-processing-2-mFR9zZTH-py3.10/lib/python3.10/site-packages/torch/autograd/function.py:506\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    504\u001b[0m     \u001b[39m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    505\u001b[0m     args \u001b[39m=\u001b[39m _functorch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 506\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mapply(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39msetup_context \u001b[39m==\u001b[39m _SingleLevelFunction\u001b[39m.\u001b[39msetup_context:\n\u001b[1;32m    509\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    510\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    511\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    512\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mstaticmethod. For more details, please see \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    513\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/Projects/git/nitro-lang-processing-2/.cache/pypoetry/virtualenvs/nitro-lang-processing-2-mFR9zZTH-py3.10/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:2151\u001b[0m, in \u001b[0;36maot_dispatch_autograd.<locals>.CompiledFunction.forward\u001b[0;34m(ctx, *deduped_flat_tensor_args)\u001b[0m\n\u001b[1;32m   2143\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m   2144\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(ctx, \u001b[39m*\u001b[39mdeduped_flat_tensor_args):\n\u001b[1;32m   2145\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2149\u001b[0m     \u001b[39m# - Note that in the synthetic bases case, mutated_inputs will correspond to an updated version\u001b[39;00m\n\u001b[1;32m   2150\u001b[0m     \u001b[39m#   of the original view, and not the synthetic base\u001b[39;00m\n\u001b[0;32m-> 2151\u001b[0m     fw_outs \u001b[39m=\u001b[39m call_func_with_args(\n\u001b[1;32m   2152\u001b[0m         CompiledFunction\u001b[39m.\u001b[39;49mcompiled_fw,\n\u001b[1;32m   2153\u001b[0m         deduped_flat_tensor_args,\n\u001b[1;32m   2154\u001b[0m         disable_amp\u001b[39m=\u001b[39;49mdisable_amp,\n\u001b[1;32m   2155\u001b[0m     )\n\u001b[1;32m   2157\u001b[0m     num_outputs \u001b[39m=\u001b[39m CompiledFunction\u001b[39m.\u001b[39mmetadata\u001b[39m.\u001b[39mnum_outputs\n\u001b[1;32m   2158\u001b[0m     num_outputs_aliased_to_inputs \u001b[39m=\u001b[39m (\n\u001b[1;32m   2159\u001b[0m         CompiledFunction\u001b[39m.\u001b[39mmetadata\u001b[39m.\u001b[39mnum_outputs_aliased_to_inputs\n\u001b[1;32m   2160\u001b[0m     )\n",
      "File \u001b[0;32m~/Projects/git/nitro-lang-processing-2/.cache/pypoetry/virtualenvs/nitro-lang-processing-2-mFR9zZTH-py3.10/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1247\u001b[0m, in \u001b[0;36mcall_func_with_args\u001b[0;34m(f, args, steal_args, disable_amp)\u001b[0m\n\u001b[1;32m   1245\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1246\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(f, \u001b[39m\"\u001b[39m\u001b[39m_boxed_call\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 1247\u001b[0m         out \u001b[39m=\u001b[39m normalize_as_list(f(args))\n\u001b[1;32m   1248\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1249\u001b[0m         \u001b[39m# TODO: Please remove soon\u001b[39;00m\n\u001b[1;32m   1250\u001b[0m         \u001b[39m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001b[39;00m\n\u001b[1;32m   1251\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1252\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mYour compiler for AOTAutograd is returning a a function that doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt take boxed arguments. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1253\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1254\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1255\u001b[0m         )\n",
      "File \u001b[0;32m~/Projects/git/nitro-lang-processing-2/.cache/pypoetry/virtualenvs/nitro-lang-processing-2-mFR9zZTH-py3.10/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:248\u001b[0m, in \u001b[0;36malign_inputs.<locals>.run\u001b[0;34m(new_inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[39mif\u001b[39;00m new_inputs[i]\u001b[39m.\u001b[39mdata_ptr() \u001b[39m%\u001b[39m ALIGNMENT:\n\u001b[1;32m    247\u001b[0m         new_inputs[i] \u001b[39m=\u001b[39m clone_preserve_strides(new_inputs[i])\n\u001b[0;32m--> 248\u001b[0m \u001b[39mreturn\u001b[39;00m model(new_inputs)\n",
      "File \u001b[0;32m/tmp/torchinductor_invokariman/cx/ccxnyq63ruknjevznouazyhmptqbrctdj5ewf6c3xiutaaapn7i7.py:3988\u001b[0m, in \u001b[0;36mcall\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m   3986\u001b[0m triton__1\u001b[39m.\u001b[39mrun(buf240, buf248, buf380, \u001b[39m786432\u001b[39m, grid\u001b[39m=\u001b[39mgrid(\u001b[39m786432\u001b[39m), stream\u001b[39m=\u001b[39mstream0)\n\u001b[1;32m   3987\u001b[0m buf249 \u001b[39m=\u001b[39m as_strided(buf240, (\u001b[39m96\u001b[39m, \u001b[39m128\u001b[39m, \u001b[39m64\u001b[39m), (\u001b[39m8192\u001b[39m, \u001b[39m64\u001b[39m, \u001b[39m1\u001b[39m)); \u001b[39mdel\u001b[39;00m buf240  \u001b[39m# reuse\u001b[39;00m\n\u001b[0;32m-> 3988\u001b[0m extern_kernels\u001b[39m.\u001b[39;49mbmm(as_strided(buf247, (\u001b[39m96\u001b[39;49m, \u001b[39m128\u001b[39;49m, \u001b[39m128\u001b[39;49m), (\u001b[39m16384\u001b[39;49m, \u001b[39m128\u001b[39;49m, \u001b[39m1\u001b[39;49m)), as_strided(buf248, (\u001b[39m96\u001b[39;49m, \u001b[39m128\u001b[39;49m, \u001b[39m64\u001b[39;49m), (\u001b[39m8192\u001b[39;49m, \u001b[39m64\u001b[39;49m, \u001b[39m1\u001b[39;49m)), out\u001b[39m=\u001b[39;49mbuf249)\n\u001b[1;32m   3989\u001b[0m buf250 \u001b[39m=\u001b[39m as_strided(buf248, (\u001b[39m1024\u001b[39m, \u001b[39m768\u001b[39m), (\u001b[39m768\u001b[39m, \u001b[39m1\u001b[39m)); \u001b[39mdel\u001b[39;00m buf248  \u001b[39m# reuse\u001b[39;00m\n\u001b[1;32m   3990\u001b[0m triton__4\u001b[39m.\u001b[39mrun(buf249, buf250, \u001b[39m786432\u001b[39m, grid\u001b[39m=\u001b[39mgrid(\u001b[39m786432\u001b[39m), stream\u001b[39m=\u001b[39mstream0)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Inspired from: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "for i, (train_idx, valid_idx) in enumerate(kf.split(train_dataset)):\n",
    "    # Split the data\n",
    "    train_subset = Subset(train_dataset, train_idx[:5000])\n",
    "    valid_subset = Subset(train_dataset, valid_idx[:1000])\n",
    "\n",
    "    # Create the dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_subset,\n",
    "        batch_size,\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        train_subset,\n",
    "        batch_size,\n",
    "    )\n",
    "\n",
    "    # ---  Training  ---\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss: List[float] = []\n",
    "        epoch_accy: List[float] = []\n",
    "\n",
    "        for batch_i, batch in enumerate(train_loader):\n",
    "            # Send batch to GPU\n",
    "            batch: Dict[str, Tensor] = { k: v.to(DEVICE) for k, v in batch.items() }\n",
    "\n",
    "            # Make predictions\n",
    "            y_true: Tensor | np.ndarray = batch['label']\n",
    "            y_pred: Tensor | np.ndarray = model.forward(batch)\n",
    "\n",
    "            # Compute the loss\n",
    "            optimizer.zero_grad()\n",
    "            loss: Tensor = loss_fn(y_pred, y_true)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Compute the accuracy\n",
    "            y_true = y_true.detach().cpu().numpy()\n",
    "            y_pred = y_pred.detach().argmax(dim=1).cpu().numpy()\n",
    "            epoch_accy.append(balanced_accuracy_score(y_true, y_pred))\n",
    "\n",
    "            # Track progress\n",
    "            epoch_loss.append(loss.detach().cpu().numpy())\n",
    "\n",
    "        mean_loss: float = np.array(epoch_loss).mean()\n",
    "        mean_accy: float = np.array(epoch_accy).mean()\n",
    "        print('Train Epoch {} - Loss: {}, Accuracy: {}'.format(epoch, mean_loss, mean_accy))\n",
    "\n",
    "    # --- Validation ---\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        epoch_loss = []\n",
    "        epoch_accy = []\n",
    "\n",
    "        for batch_i, batch in enumerate(valid_loader):\n",
    "            # Send batch to GPU\n",
    "            batch: Dict[str, Tensor] = { k: v.to(DEVICE) for k, v in batch.items() }\n",
    "\n",
    "            # Make predictions\n",
    "            y_true: Tensor | np.ndarray = batch['label']\n",
    "            y_pred: Tensor | np.ndarray = model.forward(batch)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss: Tensor = loss_fn(y_pred, y_true)\n",
    "\n",
    "            # Compute the accuracy\n",
    "            y_true = y_true.detach().cpu().numpy()\n",
    "            y_pred = y_pred.detach().argmax(dim=1).cpu().numpy()\n",
    "            epoch_accy.append(balanced_accuracy_score(y_true, y_pred))\n",
    "\n",
    "            # Track progress\n",
    "            epoch_loss.append(loss.detach().cpu().numpy())\n",
    "\n",
    "        mean_loss = np.array(epoch_loss).mean()\n",
    "        mean_accy = np.array(epoch_accy).mean()\n",
    "        print('Validation - Loss: {}, Accuracy: {}'.format(epoch, mean_loss, mean_accy))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "labels: List[str] = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_i, batch in enumerate(test_loader):\n",
    "        # Send batch to GPU\n",
    "        batch: Dict[str, Tensor] = { k: v.to(DEVICE) for k, v in batch.items() }\n",
    "\n",
    "        # Make predictions\n",
    "        y_pred: Tensor | np.ndarray = model.forward(batch)\n",
    "\n",
    "        # Compute the accuracy\n",
    "        y_pred = y_pred.detach().argmax(dim=1).cpu().numpy()\n",
    "        y_pred = np.vectorize(train_dataset.class_to_label.get)(y_pred)\n",
    "\n",
    "        # Concatenate the results\n",
    "        labels.extend(y_pred.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "output: pd.DataFrame = pd.DataFrame({ 'Label': pd.Series(data=labels) })\n",
    "output = output.reset_index()\n",
    "output = output.rename(columns={ 'index': 'Id' })\n",
    "output.to_csv(SUBMISSIONS_DIR_PATH / 'submission_5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free memory\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nitro-lang-processing-2-mFR9zZTH-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
