{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib as pb\n",
    "\n",
    "# Environment\n",
    "ROOT_PATH = pb.Path('..')\n",
    "DATA_DIR_PATH = ROOT_PATH / 'data'\n",
    "CACHE_DIR_PATH = ROOT_PATH / '.cache'\n",
    "TRANSFORMERS_CACHE_DIR_PATH = CACHE_DIR_PATH / 'transformers'\n",
    "DATASETS_CACHE_DIR_PATH = CACHE_DIR_PATH / 'datasets'\n",
    "TEST_DATA_FILE = DATA_DIR_PATH / 'test_data.csv'\n",
    "TRAIN_DATA_FILE = DATA_DIR_PATH / 'train_data.csv'\n",
    "SUBMISSIONS_DIR_PATH = ROOT_PATH / 'submissions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['TRANSFORMERS_CACHE'] = str(TRANSFORMERS_CACHE_DIR_PATH)\n",
    "os.environ['HF_DATASETS_CACHE'] = str(DATASETS_CACHE_DIR_PATH)\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n",
    "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/invokariman/Projects/git/nitro-lang-processing-2/.cache/pypoetry/virtualenvs/nitro-lang-processing-2-mFR9zZTH-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torchdata\n",
    "import torchtext\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Subset, DataLoader, Dataset, WeightedRandomSampler\n",
    "from torch import backends\n",
    "import typing\n",
    "import pathlib as pb\n",
    "import os\n",
    "import gc\n",
    "from typing import List, Tuple, Dict, Set, Callable, Any\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import platform\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score\n",
    "from balanced_loss import Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_available_device, read_data, silence_warnings\n",
    "from preprocess import BERTPreprocessor, MT5Preprocessor, RobertPreprocessor\n",
    "from data import SexismDataset\n",
    "from models import Args, Output, Ensemble, RoBertFlatClassModel, BertFlatClassModel, MT5FlatClassModel\n",
    "from train import evaluate, train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use available GPU\n",
    "DEVICE: torch.device = get_available_device()\n",
    "\n",
    "# Deterministic experiments\n",
    "SEED = 61\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "np.random.RandomState(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "# Adjust package settings\n",
    "silence_warnings()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw dataset\n",
    "train_data_raw, test_data_raw = read_data(DATA_DIR_PATH)\n",
    "\n",
    "# Initialize custom pretraiend preprocessor\n",
    "preprocessor = BERTPreprocessor()\n",
    "\n",
    "# Create and preprocess the datasets\n",
    "train_dataset = SexismDataset(train_data_raw, preprocessor)\n",
    "test_dataset = SexismDataset(test_data_raw, preprocessor)\n",
    "\n",
    "# Build common interface\n",
    "model_setup = [\n",
    "    # {\n",
    "    #     'name': 'bert',\n",
    "    #     'model_factory': lambda: BertFlatClassModel().to(DEVICE),\n",
    "    #     'optim_factory': lambda params: optim.AdamW(params, lr=Args().learning_rate, weight_decay=Args().weight_decay),\n",
    "    #     'loss_fn': nn.CrossEntropyLoss(weight=train_dataset.weights.to(DEVICE)),\n",
    "    #     'train_dataset': SexismDataset(train_data_raw, preprocessor),\n",
    "    #     'test_dataset': SexismDataset(test_data_raw, preprocessor),\n",
    "    # },\n",
    "    # {\n",
    "    #     'name': 'mt5',\n",
    "    #     'model_factory': lambda: MT5FlatClassModel().to(DEVICE),\n",
    "    #     'optim_factory': lambda params: optim.AdamW(params, lr=Args().learning_rate, weight_decay=Args().weight_decay),\n",
    "    #     'loss_fn': nn.CrossEntropyLoss(weight=train_dataset.weights.to(DEVICE)),\n",
    "    #     'train_dataset': SexismDataset(train_data_raw, MT5Preprocessor()),\n",
    "    #     'test_dataset': SexismDataset(test_data_raw, MT5Preprocessor()),\n",
    "    # },\n",
    "    {\n",
    "        'name': 'robert',\n",
    "        'model_factory': lambda: RoBertFlatClassModel().to(DEVICE),\n",
    "        'optim_factory': lambda params: optim.AdamW(params, lr=Args().learning_rate, weight_decay=Args().weight_decay),\n",
    "        'loss_fn': nn.CrossEntropyLoss(weight=train_dataset.weights.to(DEVICE)),\n",
    "        'train_dataset': SexismDataset(train_data_raw, RobertPreprocessor()),\n",
    "        'test_dataset': SexismDataset(test_data_raw, RobertPreprocessor()),\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.9444, 0.9613, 0.9944, 0.2106, 0.8893]),\n",
       " {0: tensor(0.0005),\n",
       "  1: tensor(0.0007),\n",
       "  2: tensor(0.0045),\n",
       "  3: tensor(3.2438e-05),\n",
       "  4: tensor(0.0002)})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.weights, train_dataset.class_to_freq"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep the training setup separately\n",
    "args = Args()\n",
    "model_factory: Callable[[], nn.Module] = lambda: MT5FlatClassModel().to(DEVICE)\n",
    "optim_factory: Callable[..., optim.Optimizer] = lambda params: optim.AdamW(params, lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "loss_fn = nn.CrossEntropyLoss(weight=train_dataset.weights.to(DEVICE))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HyperParameter Tuning using K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-FOLD: 0 - Model: robert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at readerbench/RoBERT-base were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 0 - Loss: 1.365743637084961, Accuracy: 0.40052489177489176\n",
      "Validation - Loss: 1.6318612098693848, Accuracy: 0.44107142857142856\n",
      "Train Epoch 1 - Loss: 0.8493609428405762, Accuracy: 0.5149702380952381\n",
      "Validation - Loss: 1.4898476600646973, Accuracy: 0.5861607142857144\n",
      "Train Epoch 2 - Loss: 0.5181704759597778, Accuracy: 0.7152083333333332\n",
      "Validation - Loss: 1.319649338722229, Accuracy: 0.6632440476190476\n",
      "Last Epoch - Loss: 0.5181704759597778, Accuracy: 0.7152083333333332\n",
      "K-FOLD: 1 - Model: robert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at readerbench/RoBERT-base were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 0 - Loss: 1.3033376932144165, Accuracy: 0.33756651334776333\n",
      "Validation - Loss: 1.4897652864456177, Accuracy: 0.35277777777777775\n",
      "Train Epoch 1 - Loss: 0.7163115739822388, Accuracy: 0.4375\n",
      "Validation - Loss: 1.4073853492736816, Accuracy: 0.47604166666666664\n",
      "Train Epoch 2 - Loss: 0.5639907717704773, Accuracy: 0.559375\n",
      "Validation - Loss: 1.2712986469268799, Accuracy: 0.6364583333333333\n",
      "Last Epoch - Loss: 0.5639907717704773, Accuracy: 0.559375\n",
      "K-FOLD: 2 - Model: robert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at readerbench/RoBERT-base were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 0 - Loss: 1.1892975568771362, Accuracy: 0.40528273809523807\n",
      "Validation - Loss: 1.4128891229629517, Accuracy: 0.39374999999999993\n",
      "Train Epoch 1 - Loss: 0.7976636290550232, Accuracy: 0.5086309523809525\n",
      "Validation - Loss: 1.3154172897338867, Accuracy: 0.6479166666666667\n",
      "Train Epoch 2 - Loss: 0.5463498830795288, Accuracy: 0.7204166666666667\n",
      "Validation - Loss: 1.1941900253295898, Accuracy: 0.7041666666666666\n",
      "Last Epoch - Loss: 0.5463498830795288, Accuracy: 0.7204166666666667\n",
      "K-FOLD: 3 - Model: robert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at readerbench/RoBERT-base were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 0 - Loss: 1.2648372650146484, Accuracy: 0.331073717948718\n",
      "Validation - Loss: 1.5429790019989014, Accuracy: 0.3944940476190476\n",
      "Train Epoch 1 - Loss: 0.8548659086227417, Accuracy: 0.4451388888888889\n",
      "Validation - Loss: 1.455643653869629, Accuracy: 0.5020833333333333\n",
      "Train Epoch 2 - Loss: 0.5925307273864746, Accuracy: 0.6214475108225108\n",
      "Validation - Loss: 1.2834300994873047, Accuracy: 0.6020833333333333\n",
      "Last Epoch - Loss: 0.5925307273864746, Accuracy: 0.6214475108225108\n",
      "K-FOLD: 4 - Model: robert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at readerbench/RoBERT-base were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 0 - Loss: 1.3183951377868652, Accuracy: 0.3341612554112554\n",
      "Validation - Loss: 1.511131763458252, Accuracy: 0.3291666666666666\n",
      "Train Epoch 1 - Loss: 0.9492776393890381, Accuracy: 0.5072420634920635\n",
      "Validation - Loss: 1.4610319137573242, Accuracy: 0.6315972222222221\n",
      "Train Epoch 2 - Loss: 0.6164517402648926, Accuracy: 0.7032589285714286\n",
      "Validation - Loss: 1.3270971775054932, Accuracy: 0.7041666666666666\n",
      "Last Epoch - Loss: 0.6164517402648926, Accuracy: 0.7032589285714286\n"
     ]
    }
   ],
   "source": [
    "# Inspired from: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
    "n_splits = 5\n",
    "kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "models: List[nn.Module] = []\n",
    "\n",
    "for i, (train_idx, valid_idx) in enumerate(kf.split(train_dataset, train_dataset.classes)):\n",
    "    # Map to dynamic model\n",
    "    setup = model_setup[i % len(model_setup)]\n",
    "    print('K-FOLD: {} - Model: {}'.format(i, setup['name']))\n",
    "\n",
    "    # Reinitialize the model\n",
    "    model: nn.Module = setup['model_factory']()\n",
    "    optimizer: optim.Optimizer = setup['optim_factory'](model.parameters())\n",
    "\n",
    "    # Split the data\n",
    "    train_subset = Subset(setup['train_dataset'], train_idx[:256])\n",
    "    valid_subset = Subset(setup['train_dataset'], valid_idx[:64])\n",
    "\n",
    "    # Inverse frequency count for random sampling\n",
    "    inv_freq = torch.tensor(list(train_dataset.class_to_freq.get(int(t['label'])) for t in train_subset))\n",
    "\n",
    "    # Create WeightedRandomSampler\n",
    "    wrs = WeightedRandomSampler(inv_freq, len(train_subset), replacement=True)\n",
    "\n",
    "    # Create the dataloaders\n",
    "    train_loader = DataLoader(train_subset, args.batch_size, sampler=wrs)\n",
    "    valid_loader = DataLoader(train_subset, args.batch_size)\n",
    "\n",
    "    # --- Validation ---\n",
    "    def validation_pass() -> None:\n",
    "        # --- Validation ---\n",
    "        valid_output: Output = evaluate(\n",
    "            model=model,\n",
    "            loss_fn=loss_fn,\n",
    "            data_loader=valid_loader,\n",
    "            with_labels=True,\n",
    "            class_to_label=train_dataset.class_to_label,\n",
    "            device=DEVICE\n",
    "        )\n",
    "        print('Validation - Loss: {}, Accuracy: {}'.format(valid_output.loss_mean, valid_output.accy_mean))\n",
    "\n",
    "    # --- Training ---\n",
    "    train_output: Output = train(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        loss_fn=loss_fn,\n",
    "        data_loader=train_loader,\n",
    "        class_to_label=train_dataset.class_to_label,\n",
    "        args=args,\n",
    "        device=DEVICE,\n",
    "        valid_callback=validation_pass,\n",
    "    )\n",
    "    print('Last Epoch - Loss: {}, Accuracy: {}'.format(train_output.loss_mean, train_output.accy_mean))\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "majority = Ensemble(models)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on the Whole Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type t5 to instantiate a model of type mt5. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at dumitrescustefan/mt5-base-romanian were not used when initializing MT5EncoderModel: ['decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.4.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.0.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.2.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.embed_tokens.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'lm_head.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.2.DenseReluDense.wi_1.weight', 'decoder.final_layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.11.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.6.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.6.layer.2.layer_norm.weight']\n",
      "- This IS expected if you are initializing MT5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MT5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Reinitialize the model\n",
    "model: nn.Module = model_factory()\n",
    "optimizer: optim.Optimizer = optim_factory(model.parameters())\n",
    "\n",
    "# Create the dataloaders\n",
    "train_loader = DataLoader(train_dataset, args.batch_size, shuffle=True)\n",
    "\n",
    "# --- Training ---\n",
    "train_output: Output = train(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    loss_fn=loss_fn,\n",
    "    data_loader=train_loader,\n",
    "    class_to_label=train_dataset.class_to_label,\n",
    "    args=args,\n",
    "    device=DEVICE,\n",
    ")\n",
    "print('Last Epoch - Loss: {}, Accuracy: {}'.format(train_output.loss_mean, train_output.accy_mean))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader: DataLoader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "test_output: Output = evaluate(\n",
    "    model=majority,\n",
    "    loss_fn=loss_fn,\n",
    "    data_loader=test_loader,\n",
    "    with_labels=False,\n",
    "    class_to_label=train_dataset.class_to_label,\n",
    "    device=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output: pd.DataFrame = pd.DataFrame({ 'Label': pd.Series(data=test_output.predictions_as_text) })\n",
    "output = output.reset_index()\n",
    "output = output.rename(columns={ 'index': 'Id' })\n",
    "output.to_csv(SUBMISSIONS_DIR_PATH / 'submission_18.csv', index=False)\n",
    "output['Label'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free memory\n",
    "# del model\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nitro-lang-processing-2-mFR9zZTH-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
