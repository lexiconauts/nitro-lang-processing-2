{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib as pb\n",
    "\n",
    "# Environment\n",
    "ROOT_PATH = pb.Path('..')\n",
    "DATA_DIR_PATH = ROOT_PATH / 'data'\n",
    "CACHE_DIR_PATH = ROOT_PATH / '.cache'\n",
    "TRANSFORMERS_CACHE_DIR_PATH = CACHE_DIR_PATH / 'transformers'\n",
    "DATASETS_CACHE_DIR_PATH = CACHE_DIR_PATH / 'datasets'\n",
    "TEST_DATA_FILE = DATA_DIR_PATH / 'test_data.csv'\n",
    "TRAIN_DATA_FILE = DATA_DIR_PATH / 'train_data.csv'\n",
    "SUBMISSIONS_DIR_PATH = ROOT_PATH / 'submissions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['TRANSFORMERS_CACHE'] = str(TRANSFORMERS_CACHE_DIR_PATH)\n",
    "os.environ['HF_DATASETS_CACHE'] = str(DATASETS_CACHE_DIR_PATH)\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/invokariman/Projects/git/nitro-lang-processing-2/.cache/pypoetry/virtualenvs/nitro-lang-processing-2-mFR9zZTH-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torchdata\n",
    "import torchtext\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Subset, DataLoader, Dataset\n",
    "from torch import backends\n",
    "import typing\n",
    "import pathlib as pb\n",
    "import os\n",
    "import gc\n",
    "from typing import List, Tuple, Dict, Set, Callable, Any\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import platform\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_available_device, read_data, silence_warnings\n",
    "from preprocess import BERTPreprocessor\n",
    "from data import SexismDataset\n",
    "from models import Args, Output, BertFlatClassModel\n",
    "from train import evaluate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use available GPU\n",
    "DEVICE: torch.device = get_available_device()\n",
    "\n",
    "# Deterministic experiments\n",
    "SEED = 61\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "np.random.RandomState(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "# Adjust package settings\n",
    "silence_warnings()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw dataset\n",
    "train_data_raw, test_data_raw = read_data(DATA_DIR_PATH)\n",
    "\n",
    "# Initialize custom pretraiend preprocessor\n",
    "preprocessor = BERTPreprocessor()\n",
    "\n",
    "# Create and preprocess the datasets\n",
    "train_dataset = SexismDataset(train_data_raw, preprocessor)\n",
    "test_dataset = SexismDataset(test_data_raw, preprocessor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep the training setup separately\n",
    "args = Args()\n",
    "model_factory: Callable[[], nn.Module] = lambda: torch.compile(BertFlatClassModel(unfreeze='none')).to(DEVICE)\n",
    "optim_factory: Callable[..., optim.Optimizer] = lambda params: optim.AdamW(params, lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "loss_fn = nn.CrossEntropyLoss(weight=train_dataset.weights.to(DEVICE))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HyperParameter Tuning using K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SexismDataset' object has no attribute 'dataset_pro'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Inspired from: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\u001b[39;00m\n\u001b[1;32m      2\u001b[0m kf \u001b[39m=\u001b[39m StratifiedKFold(n_splits\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, random_state\u001b[39m=\u001b[39mSEED)\n\u001b[0;32m----> 5\u001b[0m \u001b[39mfor\u001b[39;00m i, (train_idx, valid_idx) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(kf\u001b[39m.\u001b[39msplit(train_dataset, train_dataset\u001b[39m.\u001b[39mclasses)):\n\u001b[1;32m      6\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mK-FOLD: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(i))\n\u001b[1;32m      8\u001b[0m     \u001b[39m# Reinitialize the model\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/git/nitro-lang-processing-2/.cache/pypoetry/virtualenvs/nitro-lang-processing-2-mFR9zZTH-py3.10/lib/python3.10/site-packages/sklearn/model_selection/_split.py:342\u001b[0m, in \u001b[0;36m_BaseKFold.split\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msplit\u001b[39m(\u001b[39mself\u001b[39m, X, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, groups\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    319\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Generate indices to split data into training and test set.\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \n\u001b[1;32m    321\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[39m        The testing set indices for that split.\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 342\u001b[0m     X, y, groups \u001b[39m=\u001b[39m indexable(X, y, groups)\n\u001b[1;32m    343\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(X)\n\u001b[1;32m    344\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_splits \u001b[39m>\u001b[39m n_samples:\n",
      "File \u001b[0;32m~/Projects/git/nitro-lang-processing-2/.cache/pypoetry/virtualenvs/nitro-lang-processing-2-mFR9zZTH-py3.10/lib/python3.10/site-packages/sklearn/utils/validation.py:443\u001b[0m, in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \n\u001b[1;32m    426\u001b[0m \u001b[39mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[39m    sparse matrix, or dataframe) or `None`.\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    442\u001b[0m result \u001b[39m=\u001b[39m [_make_indexable(X) \u001b[39mfor\u001b[39;00m X \u001b[39min\u001b[39;00m iterables]\n\u001b[0;32m--> 443\u001b[0m check_consistent_length(\u001b[39m*\u001b[39;49mresult)\n\u001b[1;32m    444\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Projects/git/nitro-lang-processing-2/.cache/pypoetry/virtualenvs/nitro-lang-processing-2-mFR9zZTH-py3.10/lib/python3.10/site-packages/sklearn/utils/validation.py:394\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_consistent_length\u001b[39m(\u001b[39m*\u001b[39marrays):\n\u001b[1;32m    384\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Check that all arrays have consistent first dimensions.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[39m    Checks whether all objects in arrays have the same shape or length.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[39m        Objects that will be checked for consistent length.\u001b[39;00m\n\u001b[1;32m    392\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 394\u001b[0m     lengths \u001b[39m=\u001b[39m [_num_samples(X) \u001b[39mfor\u001b[39;00m X \u001b[39min\u001b[39;00m arrays \u001b[39mif\u001b[39;00m X \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m]\n\u001b[1;32m    395\u001b[0m     uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[1;32m    396\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/Projects/git/nitro-lang-processing-2/.cache/pypoetry/virtualenvs/nitro-lang-processing-2-mFR9zZTH-py3.10/lib/python3.10/site-packages/sklearn/utils/validation.py:394\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_consistent_length\u001b[39m(\u001b[39m*\u001b[39marrays):\n\u001b[1;32m    384\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Check that all arrays have consistent first dimensions.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[39m    Checks whether all objects in arrays have the same shape or length.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[39m        Objects that will be checked for consistent length.\u001b[39;00m\n\u001b[1;32m    392\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 394\u001b[0m     lengths \u001b[39m=\u001b[39m [_num_samples(X) \u001b[39mfor\u001b[39;00m X \u001b[39min\u001b[39;00m arrays \u001b[39mif\u001b[39;00m X \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m]\n\u001b[1;32m    395\u001b[0m     uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[1;32m    396\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/Projects/git/nitro-lang-processing-2/.cache/pypoetry/virtualenvs/nitro-lang-processing-2-mFR9zZTH-py3.10/lib/python3.10/site-packages/sklearn/utils/validation.py:344\u001b[0m, in \u001b[0;36m_num_samples\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[39mreturn\u001b[39;00m x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m    343\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 344\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlen\u001b[39;49m(x)\n\u001b[1;32m    345\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m type_error:\n\u001b[1;32m    346\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(message) \u001b[39mfrom\u001b[39;00m \u001b[39mtype_error\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/git/nitro-lang-processing-2/src/data.py:51\u001b[0m, in \u001b[0;36mSexismDataset.__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__len__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[0;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset_pro)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SexismDataset' object has no attribute 'dataset_pro'"
     ]
    }
   ],
   "source": [
    "# Inspired from: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "\n",
    "for i, (train_idx, valid_idx) in enumerate(kf.split(train_dataset, train_dataset.classes)):\n",
    "    print('K-FOLD: {}'.format(i))\n",
    "    \n",
    "    # Reinitialize the model\n",
    "    model: nn.Module = model_factory()\n",
    "    optimizer: optim.Optimizer = optim_factory(model.parameters())\n",
    "\n",
    "    # Split the data\n",
    "    train_subset = Subset(train_dataset, train_idx)\n",
    "    valid_subset = Subset(train_dataset, valid_idx)\n",
    "\n",
    "    # Create the dataloaders\n",
    "    train_loader = DataLoader(train_subset, args.batch_size)\n",
    "    valid_loader = DataLoader(train_subset, args.batch_size)\n",
    "\n",
    "    # ---  Training  ---\n",
    "    for epoch in range(args.num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss: List[float] = []\n",
    "        epoch_accy: List[float] = []\n",
    "\n",
    "        for batch_i, batch in enumerate(train_loader):\n",
    "            # Send batch to GPU\n",
    "            batch: Dict[str, Tensor] = { k: v.to(DEVICE) for k, v in batch.items() }\n",
    "\n",
    "            # Make predictions\n",
    "            y_true: Tensor | np.ndarray = batch['label']\n",
    "            y_pred: Tensor | np.ndarray = model.forward(batch)\n",
    "\n",
    "            # Compute the loss\n",
    "            optimizer.zero_grad()\n",
    "            loss: Tensor = loss_fn(y_pred, y_true)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Compute the accuracy\n",
    "            y_true = y_true.detach().cpu().numpy()\n",
    "            y_pred = y_pred.detach().argmax(dim=1).cpu().numpy()\n",
    "            epoch_accy.append(balanced_accuracy_score(y_true, y_pred))\n",
    "\n",
    "            # Track progress\n",
    "            epoch_loss.append(loss.detach().cpu().numpy())\n",
    "\n",
    "        mean_loss: float = np.array(epoch_loss).mean()\n",
    "        mean_accy: float = np.array(epoch_accy).mean()\n",
    "        print('Train Epoch {} - Loss: {}, Accuracy: {}'.format(epoch, mean_loss, mean_accy))\n",
    "\n",
    "        # --- Validation ---\n",
    "        valid_output: Output = evaluate(\n",
    "            model=model,\n",
    "            loss_fn=loss_fn,\n",
    "            data_loader=valid_loader,\n",
    "            with_labels=True,\n",
    "            device=DEVICE\n",
    "        )\n",
    "        print('Validation - Loss: {}, Accuracy: {}'.format(valid_output.loss_mean, valid_output.accy_mean))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on the Whole Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader: DataLoader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "test_output: Output = evaluate(\n",
    "    model=model,\n",
    "    loss_fn=loss_fn,\n",
    "    data_loader=test_loader,\n",
    "    with_labels=False,\n",
    "    device=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output: pd.DataFrame = pd.DataFrame({ 'Label': pd.Series(data=test_output.predictions_as_text) })\n",
    "output = output.reset_index()\n",
    "output = output.rename(columns={ 'index': 'Id' })\n",
    "output.to_csv(SUBMISSIONS_DIR_PATH / 'submission_8.csv', index=False)\n",
    "output['Label'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free memory\n",
    "# del model\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nitro-lang-processing-2-mFR9zZTH-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
